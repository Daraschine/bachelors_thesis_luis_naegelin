{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bachelors Thesis: News Sentiment and Inflation Expectation\n",
    "Luis Nägelin\n",
    "\n",
    "19-613-926\n",
    "\n",
    "Gallusstrasse 41, 9000 St.Gallen\n",
    "\n",
    "luis.naegelin@student.unisg.ch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disclaimer and declaration of autorship:\n",
    "\n",
    "The following code has been written by me (Luis Nägelin) without the direct help of any other person.\n",
    "\n",
    "I have used tools like Stack-overflow and ChatGPT to write the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packges\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "import nltk\n",
    "import pysentiment2 as ps\n",
    "lm = ps.LM() # import the Loughran McDonald dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to metadata:\n",
    "my_path = '###########' # path to the data -> folder: NYT_metadata_text\n",
    "save_path = '########'  # path to save the outputs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the data\n",
    "\n",
    "# path: path to directory were the data is stored\n",
    "# month: string of the month to load\n",
    "# year: string of the year to load\n",
    "\n",
    "def load_metadata(path, month, year):\n",
    "    filename = year +'/NYT_metadata' + year + '_' + month + '.csv'\n",
    "    data = pd.read_csv(path+filename, index_col=0)\n",
    "    print('loaded: ', filename)\n",
    "    clear_output(wait=True)\n",
    "    return data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### because diferent parts of this script were preformed at diferent stages of the analysis I have two functions that load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data Function\n",
    "def load_data(path, file_name):\n",
    "    # Concatenate the file name and the '.csv' extension\n",
    "    filename = file_name + '.csv'\n",
    "    # Read the CSV file located at the specified path, using the concatenated filename as the file to be loaded\n",
    "    # The index_col=0 argument sets the first column of the CSV file as the index column\n",
    "    # Uncomment the following line and use it if you want to load the 'keywords' column as lists\n",
    "    # converters={'keywords': lambda x: x.strip(\"[]\").replace('\"', \"'\").replace(\"'\",\"\").split(', ')}\n",
    "    data = pd.read_csv(path + filename, index_col=0)\n",
    "    \n",
    "    # Clear the output to provide a cleaner display\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    # Print the name of the loaded file\n",
    "    print('loaded:', filename)\n",
    "    \n",
    "    # Return the loaded data\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_relevant_articles(data):\n",
    "    # Filter the 'data' DataFrame to include only rows where the 'keywords' column contains the strings 'United States' or 'UNITED STATES'\n",
    "    data = data[data['keywords'].str.contains('United States|UNITED STATES', na=False)]\n",
    "\n",
    "    # Define a list of relevant article types\n",
    "    relevant_type_material = ['News', 'Letter', 'Op-Ed', 'Editorial', 'Brief']\n",
    "\n",
    "    # Filter the 'data' DataFrame to include only rows where the 'type_material' column contains any of the relevant article types\n",
    "    data = data[data['type_material'].str.contains('|'.join(relevant_type_material), na=False)]\n",
    "\n",
    "    # Return the filtered 'data' DataFrame\n",
    "    return data\n",
    "\n",
    "\n",
    "def filter_for_inflation(data):\n",
    "    # Define a list of keywords related to inflation\n",
    "    keywords = ['price index', 'price-index', 'price level', 'inflation', 'deflation', 'rising cost', 'rising costs',\n",
    "                'falling cost', 'falling costs', 'rising prices', 'price surge', 'falling prices', 'price-hike',\n",
    "                'price hike']\n",
    "\n",
    "    # Filter the 'data' DataFrame to include only rows where the 'text' column contains any of the keywords\n",
    "    data = data[data['text'].str.contains('|'.join(keywords), na=False)]\n",
    "\n",
    "    # Return the filtered 'data' DataFrame\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_articles(base_path, base_name, df, year, month):\n",
    "    # base_name = how to name the file (string)\n",
    "    # This function creates a specific folder for each year and stores the articles per month in a separate CSV file.\n",
    "    name_months = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
    "\n",
    "    # Define the path for the folder based on the base path and year\n",
    "    path = base_path + year\n",
    "\n",
    "    # Check if the folder for that year exists, if not, create it\n",
    "    isExist = os.path.exists(path)\n",
    "    if not isExist:\n",
    "        os.makedirs(path)\n",
    "\n",
    "    # Define the name of the CSV file based on the base name, year, and month\n",
    "    file_name = base_name + year + '_' + name_months[month] + '.csv'\n",
    "\n",
    "    # Store the DataFrame as a CSV file in the specified path and file name\n",
    "    df.to_csv(path + '/' + file_name)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count total number of entries in data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the total of all articles\n",
    "count = 0\n",
    "name_months = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
    "# apply to all of the data\n",
    "for year in range(1980, 2023):\n",
    "    year = str(year)\n",
    "    for month in range(0,12):\n",
    "        data = load_metadata(my_path, name_months[month], year)\n",
    "        count += len(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Title, Abstract and First Paragraph to one Text\n",
    "In Order to get the most sentiment-information out of the data, it makes sence to combine the text data from the Title, the Abstract and the first Paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_text(text):\n",
    "    # combine the Title, Abstract, First paragraph and text snippet without duplicate sentences and NaN's\n",
    "    \n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace specific characters and patterns with '.'\n",
    "    text = text.replace(';', '.').replace(' (s)', '.').replace(' (m)', '.').replace('  ', ' ').replace(':', '.')\n",
    "\n",
    "    # Tokenize the text into sentences using NLTK's sent_tokenize\n",
    "    text = nltk.sent_tokenize(text)\n",
    "\n",
    "    # Remove spaces from each sentence in text\n",
    "    no_spaces = []\n",
    "    for sentence in text:\n",
    "        no_spaces.append(sentence.replace(' ', ''))\n",
    "\n",
    "    # Keep track of unique sentences using a set and a list\n",
    "    unique_set = set()\n",
    "    unique_list = []\n",
    "\n",
    "    for i, s in enumerate(no_spaces):\n",
    "        if s not in unique_set:\n",
    "            unique_list.append(text[i])\n",
    "            unique_set.add(s)\n",
    "\n",
    "    # Clean up the unique sentences by removing 'nan' and extra periods\n",
    "    unique_text = [word.replace('nan', '').replace('.', '').replace(' .', '').replace('. ', '').replace(' . ', '').replace('.  ', '')  for word in unique_list]\n",
    "    \n",
    "    # Join the unique sentences into a single string with spaces\n",
    "    unique_text = ' '.join(unique_list)\n",
    "    \n",
    "    return unique_text\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine and Store\n",
    "This block loads all the data and adds the \"text\" collum to the data.frame by combining the title, the abstract and the first paragraph with the special \"unique_text\" function that removes duplicated sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_months = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
    "\n",
    "# Apply unique_text function to all data\n",
    "for year in range(1980, 2023):\n",
    "    year = str(year)\n",
    "    for month in range(0, 12):\n",
    "        # Load metadata for the given year and month\n",
    "        data = load_metadata(my_path, name_months[month], year)\n",
    "\n",
    "        # Create a new 'text' column by combining the 'title', 'abstract', and 'paragraph' columns with appropriate separators\n",
    "        text = data['title'].astype(str) + ' . ' + data['abstract'].astype(str) + ' . ' + data['paragraph'].astype(str) + ' . '\n",
    "\n",
    "        # Apply the unique_text function to the 'text' column to remove duplicate sentences and clean the text\n",
    "        data['text'] = text.apply(unique_text)\n",
    "\n",
    "        # Save the articles to a separate CSV file for the given year and month\n",
    "        save_articles(save_path, 'NYT_metadata', data, year, month)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the quality of the new \"text\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "data = load_metadata(my_path, 'January', '1990')\n",
    "# execute clean data and create new collum: text\n",
    "text = data['title'].astype(str) + ' . ' + data['abstract'].astype(str)+ ' . ' + data['paragraph'].astype(str)+ ' . '\n",
    "data['text'] = text.apply(unique_text)\n",
    "\n",
    "data = filter_relevant_articles(filter_for_inflation(data))\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Producer Prices Up 4.8% in '89, Most in 8 Years\n",
      "LEAD: Paced by big increases for energy and tobacco, producer prices jumped seven-tenths of 1 percent in December to bring 1989 inflation by this measure to 4.8 percent, the highest rate since 1981.\n",
      "Paced by big increases for energy and tobacco, producer prices jumped seven-tenths of 1 percent in December to bring 1989 inflation by this measure to 4.8 percent, the highest rate since 1981.\n",
      "----------\n",
      "producer prices up 4.8% in '89, most in 8 years . lead. paced by big increases for energy and tobacco, producer prices jumped seven-tenths of 1 percent in december to bring 1989 inflation by this measure to 4.8 percent, the highest rate since 1981. .\n",
      "----------\n",
      "['produc', 'lead', 'big', 'increas', 'energi', 'tobacco', 'produc', 'jump', 'percent', 'bring', 'inflat', 'measur', 'percent', 'highest', 'rate']\n",
      "{'Positive': 2, 'Negative': 0, 'Polarity': 0.99999950000025, 'Subjectivity': 0.13333332444444504}\n"
     ]
    }
   ],
   "source": [
    "# Test the new full text\n",
    "row = 1\n",
    "\n",
    "print(data['title'][row])\n",
    "print(data['abstract'][row])\n",
    "print(data['paragraph'][row])\n",
    "print('----------')\n",
    "print(data['text'][row])\n",
    "print('----------')\n",
    "print(lm.tokenize(data['text'][row]))\n",
    "print(lm.get_score(lm.tokenize(data['text'][row])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at frequency of articles of diferent types (type_material)\n",
    "In this section I analyse the diferent types of entries that make up the data set. The goal is to habe a understanding of the data and find relefant articles for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over all the data and store the frequency of diferent numbers of intrest:\n",
    "\n",
    "y_start = 1980\n",
    "y_end = 2023\n",
    "\n",
    "frequ_total_entrys = np.array([])\n",
    "frequ_relevant_world = np.array([])\n",
    "frequ_US_articles = np.array([])\n",
    "frequ_relevant_articles = np.array([])\n",
    "frequ_news = np.array([])\n",
    "frequ_letter = np.array([])\n",
    "frequ_oped = np.array([])\n",
    "frequ_editorial = np.array([])\n",
    "frequ_brief = np.array([])\n",
    "frequ_about_inflation = np.array([])\n",
    "\n",
    "# Iterate over the years and months\n",
    "for y in range(y_start, y_end):\n",
    "    year = str(y)\n",
    "    for i in range(0, 12):\n",
    "        month = name_months[i]\n",
    "        \n",
    "        my_filename = year + '/NYT_metadata' + year + '_' + month\n",
    "        data = load_data(my_path, my_filename)\n",
    "        \n",
    "        # Store the total number of entries for each month and year\n",
    "        frequ_total_entrys = np.append(frequ_total_entrys, len(data))\n",
    "        \n",
    "        # Filter the data for articles containing 'United States' keywords\n",
    "        data_US = data[data['keywords'].str.contains('United States|UNITED STATES', na=False)]\n",
    "        # Store the number of articles related to the United States for each month and year\n",
    "        frequ_US_articles = np.append(frequ_US_articles, len(data_US))\n",
    "        \n",
    "        # Filter the data for relevant articles based on specific criteria\n",
    "        data_relevant_world = filter_relevant_articles(data)\n",
    "        # Store the number of relevant articles globally for each month and year\n",
    "        frequ_relevant_world = np.append(frequ_relevant_world, len(data_relevant_world))\n",
    "        \n",
    "        # Filter the data for relevant articles specifically related to the United States\n",
    "        data_relevant_US = data_US[data_US['keywords'].str.contains('United States|UNITED STATES', na=False)]\n",
    "        # Store the number of relevant articles related to the United States for each month and year\n",
    "        frequ_relevant_articles = np.append(frequ_relevant_articles, len(data_relevant_US))\n",
    "        \n",
    "        # Store the number of articles of each type (News, Letter, Op-Ed, Editorial, Brief) for each month and year\n",
    "        frequ_news = np.append(frequ_news, len(data_relevant_US[data_relevant_US['type_material'] == 'News']))\n",
    "        frequ_letter = np.append(frequ_letter, len(data_relevant_US[data_relevant_US['type_material'] == 'Letter']))\n",
    "        frequ_oped = np.append(frequ_oped, len(data_relevant_US[data_relevant_US['type_material'] == 'Op-Ed']))\n",
    "        frequ_editorial = np.append(frequ_editorial, len(data_relevant_US[data_relevant_US['type_material'] == 'Editorial']))\n",
    "        frequ_brief = np.append(frequ_brief, len(data_relevant_US[data_relevant_US['type_material'] == 'Brief']))\n",
    "        \n",
    "        # Filter the data for articles related to inflation\n",
    "        data_inflation = filter_for_inflation(data_relevant_US)\n",
    "        # Store the number of articles related to inflation for each month and year\n",
    "        frequ_about_inflation = np.append(frequ_about_inflation, len(data_inflation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to store the frequency counts of different types of articles and other statistics\n",
    "frequ_data = pd.DataFrame({\n",
    "    'Date': np.arange(y_start, y_end, 1/12),  # Create a date range for each month\n",
    "    'frequ_total_entrys': frequ_total_entrys,  # Total number of entries for each month and year\n",
    "    'frequ_relevant_world': frequ_relevant_world,  # Number of relevant articles globally for each month and year\n",
    "    'frequ_US_articles': frequ_US_articles,  # Number of articles related to the United States for each month and year\n",
    "    'frequ_relevant_articles': frequ_relevant_articles,  # Number of relevant articles related to the United States for each month and year\n",
    "    'frequ_news': frequ_news,  # Number of articles categorized as 'News' for each month and year\n",
    "    'frequ_letter': frequ_letter,  # Number of articles categorized as 'Letter' for each month and year\n",
    "    'frequ_oped': frequ_oped,  # Number of articles categorized as 'Op-Ed' for each month and year\n",
    "    'frequ_editorial': frequ_editorial,  # Number of articles categorized as 'Editorial' for each month and year\n",
    "    'frequ_brief': frequ_brief,  # Number of articles categorized as 'Brief' for each month and year\n",
    "    'frequ_about_inflation': frequ_about_inflation  # Number of articles related to inflation for each month and year\n",
    "})\n",
    "\n",
    "# Save the results to a CSV file\n",
    "frequ_data.to_csv(save_path + '/frequ_article_type.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Desk Analysis\n",
    "The Idear was to filter for all Economic articles in order to have someting to compare my filter for inflation to. (Was not used in final analysis).\n",
    "I stored all News-Desk-Keywords in a Table and manuly looked for \"Business-related\" keywords. Then I coppied them into the file: business_desk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for looking at business desk data. (Was not used in final analysis.)\n",
    "path_to_business_desk = '#####' # path to the file: business_desk.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_to_business_desk+ \"business_desk.txt\", 'r') as file:\n",
    "    business_desk_keywords = file.readlines()\n",
    "\n",
    "for i in range(len(business_desk_keywords)):\n",
    "    business_desk_keywords[i] = business_desk_keywords[i][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_news_desk(data, filterkeywords):\n",
    "    # Filter the data based on the news_desk column and keywords related to the United States\n",
    "    data = data[data['news_desk'].isin(filterkeywords)]\n",
    "    data = data[data['keywords'].str.contains('United States|UNITED STATES', na=False)]\n",
    "    return data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load all the data and filter with the busines_desk keywords, then calculate the sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sorce = 'text'\n",
    "y_start = 1980\n",
    "y_end = 2023\n",
    "\n",
    "# Define sentiment labels: positive, neutral, and negative\n",
    "# positive article = 1     \n",
    "# neutral article = 0       \n",
    "# negative article = -1     \n",
    "\n",
    "sentiment_of_articles = []  # nested list of all the counts of positive/negative/neutral articles in a given month: [num_positive, num_neutral, num_negative]\n",
    "positive_articles_per_month = np.array([])  # number of positive articles per month\n",
    "negative_articles_per_month = np.array([])  # number of negative articles per month\n",
    "neutral_articles_per_month = np.array([])  # number of neutral articles per month\n",
    "mean_sentiment_per_month = np.array([])  # mean sentiment score per month\n",
    "articles_per_month = np.array([])  # number of relevant articles in a given month\n",
    "empthy_counter = 0  # counter for empty articles\n",
    "\n",
    "# Iterate over years\n",
    "for y in range(y_start, y_end):\n",
    "    year = str(y)\n",
    "\n",
    "    # Iterate over months\n",
    "    for i in range(0, 12):\n",
    "        month = name_months[i]\n",
    "\n",
    "        # Load data and filter by news desk keywords\n",
    "        my_filename = year + '/NYT_metadata' + year + '_' + month\n",
    "        data = filter_news_desk(load_data(my_path, my_filename), business_desk_keywords)\n",
    "\n",
    "        articles_counter = 0  # relevant articles (not empty) counter\n",
    "        positive_articles = 0  # count of positive articles\n",
    "        neutral_articles = 0  # count of neutral articles\n",
    "        negative_articles = 0  # count of negative articles\n",
    "\n",
    "        # Iterate over data entries\n",
    "        for index in data.index:\n",
    "            text = data.loc[index, text_sorce]\n",
    "            if isinstance(text, str):\n",
    "                articles_counter += 1  # increment relevant articles counter\n",
    "\n",
    "                tokenized_text = lm.tokenize(text)\n",
    "                score = lm.get_score(tokenized_text)['Polarity']  # get sentiment score\n",
    "\n",
    "                if score > 0:\n",
    "                    positive_articles += 1\n",
    "                elif score < 0:\n",
    "                    negative_articles += 1\n",
    "                else:\n",
    "                    neutral_articles += 1\n",
    "\n",
    "        # Append sentiment counts to sentiment_of_articles\n",
    "        sentiment_of_articles.append(np.array([positive_articles, neutral_articles, negative_articles]))\n",
    "\n",
    "        # Save the number of relevant articles in this month\n",
    "        articles_per_month = np.append(articles_per_month, articles_counter)\n",
    "\n",
    "# Calculate means of sentiment per month\n",
    "for month in sentiment_of_articles:\n",
    "    if month.sum() == 0:\n",
    "        mean_sentiment_per_month = np.append(mean_sentiment_per_month, 0)\n",
    "        positive_articles_per_month = np.append(positive_articles_per_month, 0)\n",
    "        neutral_articles_per_month = np.append(neutral_articles_per_month, 0)\n",
    "        negative_articles_per_month = np.append(negative_articles_per_month, 0)\n",
    "    else:\n",
    "        mean = (month[0] - month[2]) / month.sum()\n",
    "        mean_sentiment_per_month = np.append(mean_sentiment_per_month, mean)\n",
    "        positive_articles_per_month = np.append(positive_articles_per_month, month[0])\n",
    "        neutral_articles_per_month = np.append(neutral_articles_per_month, month[1])\n",
    "        negative_articles_per_month = np.append(negative_articles_per_month, month[2])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
