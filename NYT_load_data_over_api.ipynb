{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bachelors Thesis: News Sentiment and Inflation Expectation\n",
    "Luis Nägelin\n",
    "\n",
    "19-613-926\n",
    "\n",
    "Gallusstrasse 41, 9000 St.Gallen\n",
    "\n",
    "luis.naegelin@student.unisg.ch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disclaimer and declaration of autorship:\n",
    "\n",
    "The following code has been written by me (Luis Nägelin) without the direct help of any other person.\n",
    "\n",
    "I have used tools like Stack-overflow and ChatGPT to write the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packges\n",
    "import pandas as pd\n",
    "import requests as req\n",
    "import time\n",
    "import os\n",
    "import nltk\n",
    "import newspaper #pip3 install newspaper3k\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters and Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path were to save the files\n",
    "my_base_path = '######'    # always change \"\\\" to \"/\" !!            Path to store data\n",
    "\n",
    "# API Key: Personal NYT API Key:\n",
    "api_key = 'ooFDYm9g0OiQjEx95tjeo67LOe1e8VNp'\n",
    "# Base URL for the NYT api\n",
    "base_url = 'https://api.nytimes.com/svc/archive/v1/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_articles(response, article_list):\n",
    "    # Extract data from Response\n",
    "    docs = response['response']['docs']\n",
    "    \n",
    "    for doc in docs:\n",
    "        filteredDoc = {}\n",
    "        \n",
    "        # Extract different values of interest from the document\n",
    "        filteredDoc['title'] = doc['headline']['main']  # Title of the article\n",
    "        filteredDoc['print_title'] = doc['headline']['print_headline']  # Print title of the article\n",
    "        filteredDoc['abstract'] = doc['abstract']  # Abstract of the article\n",
    "        filteredDoc['paragraph'] = doc['lead_paragraph']  # First paragraph of the article\n",
    "        filteredDoc['text_snippet'] = doc['snippet']  # Text snippet of the article\n",
    "        filteredDoc['keywords'] = []  # List to store keywords\n",
    "        keywords = doc.get('keywords')  # Get the list of keywords\n",
    "        for key in keywords:\n",
    "            filteredDoc['keywords'].append(key.get('value'))  # Extract the values of keywords\n",
    "        filteredDoc['date'] = doc['pub_date']  # Publication date of the article\n",
    "        filteredDoc['news_desk'] = doc['news_desk']  # News desk of the article\n",
    "        filteredDoc['section_name'] = doc['section_name']  # Section name of the article\n",
    "        filteredDoc['type_material'] = doc['type_of_material']  # Type of material of the article\n",
    "        filteredDoc['word_count'] = doc['word_count']  # Word count of the article\n",
    "        filteredDoc['web_url'] = doc['web_url']  # Web URL of the article\n",
    "        filteredDoc['uri'] = doc['uri']  # URI of the article\n",
    "        filteredDoc['id'] = doc['_id']  # ID of the article\n",
    "\n",
    "        article_list.append(filteredDoc)  # Append the filtered document to the article list\n",
    "\n",
    "    return article_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves the data on local device:\n",
    "def save_articles(base_path, base_name, df, year, month):\n",
    "    # base_name = how to name the file (string)\n",
    "    # This function creates a specific folder fo each year and stores the articles per month in a seperate csv file.\n",
    "    name_months = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
    "    \n",
    "    path = base_path + '/' + year\n",
    "    # check if folder for that year exists\n",
    "    isExist = os.path.exists(path)\n",
    "    if not isExist:\n",
    "        os.makedirs(path)\n",
    "    # name of the csv file\n",
    "    file_name = base_name + year + '_' + name_months[month-1] + '.csv'\n",
    "\n",
    "    # store data frame\n",
    "    df.to_csv(path + '/' + file_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Metadata over NYT API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list(range(1980, 2021))  # List of years\n",
    "months = list(range(1, 13))  # List of months\n",
    "\n",
    "# Iterate over years and months\n",
    "for year in years:\n",
    "    year = str(year)\n",
    "    for month in months:\n",
    "        articles = []  # List to store the filtered articles\n",
    "        request_url = base_url + '/' + year + '/' + str(month) + '.json?api-key=' + api_key\n",
    "        # Send a GET request to the API to fetch the articles for the specified year and month\n",
    "        my_response = req.get(request_url)\n",
    "\n",
    "        # Retry if the response is not 200 (OK)\n",
    "        counter = 0\n",
    "        while my_response.status_code != 200:\n",
    "            counter += 1\n",
    "            time.sleep(5)  # Wait for 5 seconds before retrying\n",
    "            my_response = req.get(request_url)\n",
    "\n",
    "            if counter > 4:\n",
    "                break\n",
    "\n",
    "        if my_response.status_code == 200:\n",
    "            # Use the filter_articles() function to filter out relevant information from the response\n",
    "            articles = filter_articles(my_response.json(), articles)\n",
    "            # Convert the filtered articles to a DataFrame\n",
    "            df = pd.DataFrame(articles)\n",
    "            # Use the save_articles() function to save the DataFrame as a CSV file\n",
    "            save_articles(my_base_path, 'NYT_metadata', df, year, month)\n",
    "        else:\n",
    "            print('Something was wrong with', str(month), year)\n",
    "\n",
    "        # Add a delay of 2 seconds to avoid hitting the API request limit\n",
    "        time.sleep(2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
